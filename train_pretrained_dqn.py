"""
Script d'entra√Ænement DQN avec pr√©-entra√Ænement sur le point optimal
"""

import argparse
import os
import time
from typing import Any

import matplotlib.pyplot as plt
import numpy as np
from magic_pong.ai.models.dqn_ai import DQNAgent
from magic_pong.ai.models.simple_ai import create_ai
from magic_pong.ai.pretraining import create_pretrainer
from magic_pong.core.game_engine import TrainingManager
from magic_pong.utils.config import ai_config
from magic_pong.utils.config import game_config


class DQNPretrainer:
    """Gestionnaire d'entra√Ænement DQN avec pr√©-entra√Ænement sur le point optimal"""

    def __init__(
        self,
        episodes: int = 1000,
        pretraining_steps: int = 10000,
        save_interval: int = 100,
        eval_interval: int = 50,
        eval_episodes: int = 10,
        model_dir: str = "models",
    ):
        """
        Args:
            episodes: Nombre d'√©pisodes d'entra√Ænement principal
            pretraining_steps: Nombre d'√©tapes de pr√©-entra√Ænement
            save_interval: Intervalle de sauvegarde du mod√®le
            eval_interval: Intervalle d'√©valuation
            eval_episodes: Nombre d'√©pisodes d'√©valuation
            model_dir: R√©pertoire de sauvegarde des mod√®les
        """
        self.episodes = episodes
        self.pretraining_steps = pretraining_steps
        self.save_interval = save_interval
        self.eval_interval = eval_interval
        self.eval_episodes = eval_episodes
        self.model_dir = model_dir

        # Cr√©er le r√©pertoire des mod√®les
        os.makedirs(model_dir, exist_ok=True)

        # M√©triques d'entra√Ænement
        self.training_rewards = []
        self.pretraining_rewards = []
        self.win_rates = []

        # Pour la reprise d'entra√Ænement
        self.start_episode = 0
        self.best_avg_reward = float("-inf")
        self.pretraining_completed = False

    def run_pretraining_phase(
        self,
        agent: DQNAgent,
        player_id: int = 1,
        steps_per_batch: int = 1000,
        save_pretrained_model: bool = True,
        y_only: bool = True,
    ) -> dict[str, Any]:
        """
        Ex√©cute la phase de pr√©-entra√Ænement sur la proximit√© au point optimal.

        Args:
            agent: Agent DQN √† pr√©-entra√Æner
            player_id: ID du joueur (1 pour gauche, 2 pour droite)
            steps_per_batch: Nombre d'√©tapes par batch
            save_pretrained_model: Sauvegarder le mod√®le apr√®s pr√©-entra√Ænement
            y_only: Si True, ne consid√®re que la distance verticale pour la r√©compense

        Returns:
            Statistiques du pr√©-entra√Ænement
        """
        print("üéØ === PHASE DE PR√â-ENTRA√éNEMENT ===")
        print("Objectif: Apprendre √† s'approcher du point optimal d'interception")
        print(f"√âtapes de pr√©-entra√Ænement: {self.pretraining_steps}")
        print()

        # Cr√©er le pr√©-entra√Æneur
        pretrainer = create_pretrainer(y_only=y_only)

        # Activer le mode headless pour la vitesse
        original_headless = ai_config.HEADLESS_MODE
        original_fast_mode = ai_config.FAST_MODE_MULTIPLIER
        initial_game_speed_multiplier = game_config.GAME_SPEED_MULTIPLIER
        initial_fps = game_config.FPS
        ai_config.USE_PROXIMITY_REWARD = True
        ai_config.PROXIMITY_REWARD_FACTOR = 1
        ai_config.PROXIMITY_PENALTY_FACTOR = 1
        ai_config.MAX_PROXIMITY_REWARD = 1000
        ai_config.HEADLESS_MODE = True
        ai_config.FAST_MODE_MULTIPLIER = (
            1.0  # Pas besoin de vitesse √©lev√©e pour le pr√©-entra√Ænement
        )
        game_config.GAME_SPEED_MULTIPLIER = 5.0
        game_config.FPS = 300.0

        start_time = time.time()

        try:
            # Ex√©cuter le pr√©-entra√Ænement
            pretraining_stats = pretrainer.run_pretraining_phase(
                agent=agent,
                total_steps=self.pretraining_steps,
                steps_per_batch=steps_per_batch,
                player_id=player_id,
                verbose=True,
            )

            self.pretraining_rewards = pretraining_stats["all_rewards"]
            self.pretraining_completed = True

            # Sauvegarder le mod√®le pr√©-entra√Æn√©
            if save_pretrained_model:
                pretrained_model_path = os.path.join(self.model_dir, "pretrained_optimal_point.pth")
                agent.save_model(pretrained_model_path)
                print(f"üìÅ Mod√®le pr√©-entra√Æn√© sauvegard√©: {pretrained_model_path}")

            elapsed_time = time.time() - start_time
            print(f"\n‚úÖ Pr√©-entra√Ænement termin√© en {elapsed_time:.1f}s")
            print(
                f"   Am√©lioration de la r√©compense de proximit√©: {pretraining_stats['average_reward']:.3f}"
            )
            print("   Agent pr√™t pour l'entra√Ænement principal!")

            return pretraining_stats

        finally:
            # Restaurer la configuration originale
            ai_config.HEADLESS_MODE = original_headless
            ai_config.FAST_MODE_MULTIPLIER = original_fast_mode
            game_config.GAME_SPEED_MULTIPLIER = initial_game_speed_multiplier
            game_config.FPS = initial_fps

    def train_with_pretraining(
        self,
        opponent_type: str = "follow_ball",
        agent_kwargs: dict[str, Any] | None = None,
        skip_pretraining: bool = False,
        pretraining_only: bool = False,
        resume_training: bool = False,
    ) -> DQNAgent:
        """
        Entra√Æne l'agent avec pr√©-entra√Ænement puis entra√Ænement principal.

        Args:
            opponent_type: Type d'adversaire pour l'entra√Ænement principal
            agent_kwargs: Arguments pour la cr√©ation de l'agent DQN
            skip_pretraining: Ignorer la phase de pr√©-entra√Ænement
            pretraining_only: Faire seulement le pr√©-entra√Ænement
            resume_training: Reprendre un entra√Ænement existant

        Returns:
            Agent DQN entra√Æn√©
        """
        if agent_kwargs is None:
            agent_kwargs = {}

        print("üöÄ === ENTRA√éNEMENT DQN AVEC PR√â-ENTRA√éNEMENT ===")
        print(f"Phase 1: Pr√©-entra√Ænement ({self.pretraining_steps} √©tapes)")
        print(f"Phase 2: Entra√Ænement principal ({self.episodes} √©pisodes vs {opponent_type})")
        print()

        # Ajouter la taille d'√©tat correcte si non sp√©cifi√©e
        if "state_size" not in agent_kwargs:
            agent_kwargs["state_size"] = 28  # Taille correcte pour l'√©tat √©tendu

        # Cr√©er l'agent DQN
        dqn_agent = DQNAgent(player_id=1, name="DQN_Pretrained", **agent_kwargs)

        # Phase 1: Pr√©-entra√Ænement (sauf si demand√© de l'ignorer)
        if not skip_pretraining:
            pretraining_stats = self.run_pretraining_phase(dqn_agent, y_only=True)

            # Tracer les r√©sultats du pr√©-entra√Ænement
            self.plot_pretraining_results(pretraining_stats)

            if pretraining_only:
                print("üéØ Pr√©-entra√Ænement seul termin√©!")
                return dqn_agent

        # Phase 2: Entra√Ænement principal
        print("\nü•ä === PHASE D'ENTRA√éNEMENT PRINCIPAL ===")
        print(f"Adversaire: {opponent_type}")
        print(f"√âpisodes: {self.episodes}")

        # Configuration pour l'entra√Ænement rapide
        ai_config.HEADLESS_MODE = True
        ai_config.FAST_MODE_MULTIPLIER = 10.0

        # Cr√©er l'adversaire
        opponent = create_ai(opponent_type, player_id=2, name=f"Opponent_{opponent_type}")

        # Cr√©er le gestionnaire d'entra√Ænement
        training_manager = TrainingManager(headless=True)

        # Variables pour le suivi
        episode_rewards = []
        recent_rewards = []
        best_avg_reward = self.best_avg_reward

        start_time = time.time()

        for episode in range(self.episodes):
            # Jouer un √©pisode complet
            episode_stats = training_manager.train_episode(dqn_agent, opponent, max_steps=1000)
            episode_reward = episode_stats["total_reward_p1"]

            episode_rewards.append(episode_reward)
            recent_rewards.append(episode_reward)

            # Garder seulement les 100 derniers √©pisodes
            if len(recent_rewards) > 100:
                recent_rewards.pop(0)

            # Logging p√©riodique
            if (episode + 1) % 50 == 0:
                avg_reward = np.mean(recent_rewards)
                elapsed_time = time.time() - start_time
                print(f"√âpisode {episode + 1}/{self.episodes}")
                print(f"  R√©compense moyenne (100 derniers): {avg_reward:.2f}")
                print(f"  Epsilon: {dqn_agent.epsilon:.3f}")
                print(f"  Temps √©coul√©: {elapsed_time:.1f}s")
                print(f"  √âtapes d'entra√Ænement: {dqn_agent.training_step}")

                # Sauvegarder le meilleur mod√®le
                if avg_reward > best_avg_reward:
                    best_avg_reward = avg_reward
                    self.best_avg_reward = best_avg_reward
                    model_path = os.path.join(
                        self.model_dir, f"best_pretrained_vs_{opponent_type}.pth"
                    )
                    dqn_agent.save_model(model_path)
                    print(f"  üèÜ Nouveau meilleur mod√®le sauvegard√©! R√©compense: {avg_reward:.2f}")

            # Sauvegarde p√©riodique
            if (episode + 1) % self.save_interval == 0:
                model_path = os.path.join(
                    self.model_dir, f"checkpoint_pretrained_ep{episode+1}_vs_{opponent_type}.pth"
                )
                dqn_agent.save_model(model_path)

            # √âvaluation p√©riodique
            if (episode + 1) % self.eval_interval == 0:
                win_rate = self.evaluate_agent(dqn_agent, opponent_type)
                self.win_rates.append(win_rate)
                print(f"  üìä Taux de victoire: {win_rate:.1%}")

        # Sauvegarder le mod√®le final
        final_model_path = os.path.join(self.model_dir, f"final_pretrained_vs_{opponent_type}.pth")
        dqn_agent.save_model(final_model_path)

        # Stocker les m√©triques
        self.training_rewards = episode_rewards

        print("\n‚úÖ Entra√Ænement principal termin√©!")
        print(f"Temps total: {time.time() - start_time:.1f}s")
        print(f"R√©compense moyenne finale: {np.mean(recent_rewards):.2f}")

        return dqn_agent

    def evaluate_agent(self, agent: DQNAgent, opponent_type: str) -> float:
        """√âvalue l'agent sur plusieurs parties"""
        # Mettre l'agent en mode √©valuation
        agent.set_training_mode(False)

        # Cr√©er l'adversaire
        opponent = create_ai(opponent_type, player_id=2)

        # Cr√©er le gestionnaire d'√©valuation
        eval_manager = TrainingManager(headless=True)

        wins = 0

        for _ in range(self.eval_episodes):
            # Jouer une partie compl√®te
            episode_stats = eval_manager.train_episode(agent, opponent, max_steps=1000)

            # V√©rifier qui a gagn√©
            if episode_stats["winner"] == agent.player_id:
                wins += 1

        # Remettre l'agent en mode entra√Ænement
        agent.set_training_mode(True)

        return wins / self.eval_episodes

    def plot_pretraining_results(self, pretraining_stats: dict[str, Any]) -> None:
        """Affiche les r√©sultats du pr√©-entra√Ænement"""
        rewards = pretraining_stats["all_rewards"]
        if not rewards:
            return

        plt.figure(figsize=(12, 6))

        # Graphique des r√©compenses de pr√©-entra√Ænement
        plt.subplot(1, 2, 1)
        plt.plot(rewards, alpha=0.7, color="blue", linewidth=0.8)

        # Moyenne mobile
        window_size = min(100, len(rewards) // 10)
        if window_size > 1:
            moving_avg = np.convolve(rewards, np.ones(window_size) / window_size, mode="valid")
            plt.plot(
                range(window_size - 1, len(rewards)),
                moving_avg,
                color="red",
                linewidth=2,
                label=f"Moyenne mobile ({window_size})",
            )

        plt.xlabel("√âtape de pr√©-entra√Ænement")
        plt.ylabel("R√©compense de proximit√©")
        plt.title("√âvolution pendant le pr√©-entra√Ænement")
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Histogramme des r√©compenses
        plt.subplot(1, 2, 2)
        plt.hist(rewards, bins=50, alpha=0.7, color="green", edgecolor="black")
        plt.xlabel("R√©compense de proximit√©")
        plt.ylabel("Fr√©quence")
        plt.title("Distribution des r√©compenses")
        plt.grid(True, alpha=0.3)

        # Statistiques
        stats_text = f"""Statistiques du pr√©-entra√Ænement:
R√©compense moyenne: {np.mean(rewards):.3f}
√âcart-type: {np.std(rewards):.3f}
Min: {np.min(rewards):.3f}
Max: {np.max(rewards):.3f}
√âtapes: {len(rewards)}"""

        plt.figtext(0.02, 0.02, stats_text, fontsize=10, verticalalignment="bottom")

        plt.tight_layout()
        plt.subplots_adjust(bottom=0.15)

        # Sauvegarder le graphique
        plot_path = os.path.join(self.model_dir, "pretraining_results.png")
        plt.savefig(plot_path, dpi=300, bbox_inches="tight")
        print(f"üìä Graphiques du pr√©-entra√Ænement sauvegard√©s: {plot_path}")

        plt.show()

    def plot_full_training_results(self) -> None:
        """Affiche les r√©sultats complets (pr√©-entra√Ænement + entra√Ænement principal)"""
        if not self.training_rewards and not self.pretraining_rewards:
            print("Aucune donn√©e d'entra√Ænement √† afficher")
            return

        plt.figure(figsize=(15, 10))

        # Graphique combin√© des r√©compenses
        plt.subplot(2, 2, 1)

        # Pr√©-entra√Ænement
        if self.pretraining_rewards:
            pretraining_x = np.arange(len(self.pretraining_rewards)) - len(self.pretraining_rewards)
            plt.plot(
                pretraining_x,
                self.pretraining_rewards,
                alpha=0.5,
                color="blue",
                label="Pr√©-entra√Ænement",
            )

        # Entra√Ænement principal
        if self.training_rewards:
            training_x = np.arange(len(self.training_rewards))
            plt.plot(
                training_x, self.training_rewards, alpha=0.7, color="red", label="Entra√Ænement"
            )

            # Moyenne mobile pour l'entra√Ænement
            window_size = min(50, len(self.training_rewards) // 5)
            if window_size > 1:
                moving_avg = np.convolve(
                    self.training_rewards, np.ones(window_size) / window_size, mode="valid"
                )
                plt.plot(
                    training_x[window_size - 1 :],
                    moving_avg,
                    color="darkred",
                    linewidth=2,
                    label=f"Moyenne mobile ({window_size})",
                )

        plt.axvline(x=0, color="black", linestyle="--", alpha=0.5, label="D√©but entra√Ænement")
        plt.xlabel("√âtape / √âpisode")
        plt.ylabel("R√©compense")
        plt.title("√âvolution compl√®te des r√©compenses")
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Taux de victoire
        if self.win_rates:
            plt.subplot(2, 2, 2)
            episodes_eval = np.arange(
                self.eval_interval, len(self.win_rates) * self.eval_interval + 1, self.eval_interval
            )
            plt.plot(episodes_eval, self.win_rates, "o-", color="green", linewidth=2)
            plt.xlabel("√âpisode")
            plt.ylabel("Taux de victoire")
            plt.title("√âvolution du taux de victoire")
            plt.grid(True, alpha=0.3)

        # Comparaison des histogrammes
        plt.subplot(2, 2, 3)
        if self.pretraining_rewards:
            plt.hist(
                self.pretraining_rewards,
                bins=30,
                alpha=0.5,
                color="blue",
                label="Pr√©-entra√Ænement",
                density=True,
            )
        if self.training_rewards:
            plt.hist(
                self.training_rewards,
                bins=30,
                alpha=0.5,
                color="red",
                label="Entra√Ænement",
                density=True,
            )
        plt.xlabel("R√©compense")
        plt.ylabel("Densit√©")
        plt.title("Distribution des r√©compenses")
        plt.legend()
        plt.grid(True, alpha=0.3)

        # Statistiques globales
        plt.subplot(2, 2, 4)
        stats_lines = ["Statistiques globales:\n"]

        if self.pretraining_rewards:
            stats_lines.extend(
                [
                    "Pr√©-entra√Ænement:",
                    f"  √âtapes: {len(self.pretraining_rewards)}",
                    f"  R√©compense moy.: {np.mean(self.pretraining_rewards):.3f}",
                    f"  R√©compense fin: {np.mean(self.pretraining_rewards[-100:]):.3f}",
                    "",
                ]
            )

        if self.training_rewards:
            recent_rewards = self.training_rewards[-100:]
            stats_lines.extend(
                [
                    "Entra√Ænement principal:",
                    f"  √âpisodes: {len(self.training_rewards)}",
                    f"  R√©compense moy.: {np.mean(self.training_rewards):.2f}",
                    f"  R√©compense finale: {np.mean(recent_rewards):.2f}",
                    "",
                ]
            )

        if self.win_rates:
            stats_lines.extend(
                [
                    "Performance:",
                    f"  Taux de victoire final: {self.win_rates[-1]:.1%}",
                    f"  Meilleur taux: {max(self.win_rates):.1%}",
                ]
            )

        plt.text(0.1, 0.5, "\n".join(stats_lines), fontsize=11, verticalalignment="center")
        plt.axis("off")

        plt.tight_layout()

        # Sauvegarder
        plot_path = os.path.join(self.model_dir, "full_training_results.png")
        plt.savefig(plot_path, dpi=300, bbox_inches="tight")
        print(f"üìä Graphiques complets sauvegard√©s: {plot_path}")

        plt.show()


def main():
    """Fonction principale avec pr√©-entra√Ænement"""
    parser = argparse.ArgumentParser(
        description="Entra√Ænement DQN avec pr√©-entra√Ænement sur le point optimal"
    )

    # Arguments d'entra√Ænement
    parser.add_argument(
        "--episodes", type=int, default=1000, help="Nombre d'√©pisodes d'entra√Ænement principal"
    )
    parser.add_argument(
        "--pretraining_steps",
        type=int,
        default=10000,
        help="Nombre d'√©tapes de pr√©-entra√Ænement sur le point optimal",
    )
    parser.add_argument(
        "--opponent",
        type=str,
        default="follow_ball",
        choices=["random", "follow_ball", "defensive", "aggressive", "predictive"],
        help="Type d'adversaire pour l'entra√Ænement principal",
    )

    # Arguments du r√©seau
    parser.add_argument("--lr", type=float, default=0.001, help="Taux d'apprentissage")
    parser.add_argument("--tau", type=float, default=0.005, help="Coefficient pour les soft updates du target network")
    parser.add_argument("--gamma", type=float, default=0.99, help="Facteur de discount")
    parser.add_argument(
        "--epsilon", type=float, default=1.0, help="Epsilon initial pour l'exploration"
    )
    parser.add_argument(
        "--epsilon_decay", type=float, default=0.995, help="Facteur de d√©croissance d'epsilon"
    )
    parser.add_argument(
        "--epsilon_min", type=float, default=0.01, help="Epsilon minimum pour l'exploration"
    )
    parser.add_argument("--memory_size", type=int, default=20000, help="Taille du replay buffer")
    parser.add_argument(
        "--batch_size", type=int, default=32, help="Taille des batches d'entra√Ænement"
    )

    # Arguments de contr√¥le
    parser.add_argument(
        "--skip_pretraining",
        action="store_true",
        help="Ignorer la phase de pr√©-entra√Ænement",
    )
    parser.add_argument(
        "--pretraining_only",
        action="store_true",
        help="Faire seulement la phase de pr√©-entra√Ænement",
    )
    parser.add_argument(
        "--model_dir", type=str, default="models", help="R√©pertoire de sauvegarde des mod√®les"
    )
    parser.add_argument(
        "--plot", action="store_true", help="Afficher les graphiques d'entra√Ænement"
    )

    args = parser.parse_args()

    # Cr√©er le trainer avec pr√©-entra√Ænement
    trainer = DQNPretrainer(
        episodes=args.episodes,
        pretraining_steps=args.pretraining_steps,
        model_dir=args.model_dir,
    )

    # Configuration de l'agent
    agent_kwargs = {
        "tau": args.tau,
        "lr": args.lr,
        "gamma": args.gamma,
        "epsilon": args.epsilon,
        "epsilon_decay": args.epsilon_decay,
        "epsilon_min": args.epsilon_min,
        "memory_size": args.memory_size,
        "batch_size": args.batch_size,
    }

    print("üéØ Configuration:")
    print(f"   Pr√©-entra√Ænement: {args.pretraining_steps} √©tapes")
    print(f"   Entra√Ænement: {args.episodes} √©pisodes vs {args.opponent}")
    print(f"   Sauvegarde: {args.model_dir}")
    print()

    # Entra√Æner l'agent
    trainer.train_with_pretraining(
        opponent_type=args.opponent,
        agent_kwargs=agent_kwargs,
        skip_pretraining=args.skip_pretraining,
        pretraining_only=args.pretraining_only,
    )

    # Afficher les graphiques si demand√©
    if args.plot:
        trainer.plot_full_training_results()

    print("\nüéâ Entra√Ænement avec pr√©-entra√Ænement termin√© avec succ√®s!")


if __name__ == "__main__":
    main()
