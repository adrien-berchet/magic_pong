# Magic Pong

Un jeu de Pong √©labor√© con√ßu sp√©cialement pour l'entra√Ænement d'intelligence artificielle, avec des fonctionnalit√©s avanc√©es et une architecture flexible.

## Fonctionnalit√©s

### Gameplay Avanc√©
- **Mouvement libre** : Les joueurs peuvent se d√©placer librement dans leur moiti√© de terrain (pas seulement verticalement)
- **Syst√®me de bonus sym√©triques** :
  - √âlargissement de la raquette du joueur
  - R√©tr√©cissement de la raquette adverse
  - Raquette tournante suppl√©mentaire
- **Physique r√©aliste** avec rebonds et effets

### Interface IA
- **Architecture agnostique** : Compatible avec diff√©rents frameworks d'IA (PyTorch, TensorFlow, etc.)
- **Mode headless** : Entra√Ænement ultra-rapide sans affichage graphique
- **Vitesse variable** : Acc√©l√©ration jusqu'√† 1000x pour l'entra√Ænement
- **Syst√®me de r√©compenses** configurable
- **Observations normalis√©es** pour l'apprentissage

### Exemples d'IA Inclus
- **RandomAI** : Mouvements al√©atoires
- **FollowBallAI** : Suit la balle
- **DefensiveAI** : Strat√©gie d√©fensive
- **AggressiveAI** : Cherche les bonus et attaque
- **PredictiveAI** : Pr√©dit la trajectoire de la balle

## Installation

```bash
# Cloner le projet
git clone <repository_url>
cd magic_pong

# Installer les d√©pendances
pip install -r requirements.txt
```

## Utilisation Rapide

### Entra√Ænement IA vs IA

```python
from src.core.game_engine import TrainingManager
from src.ai.examples.simple_ai import create_ai

# Cr√©er le gestionnaire d'entra√Ænement
trainer = TrainingManager(headless=True)

# Cr√©er les IA
player1 = create_ai('aggressive', 1)
player2 = create_ai('defensive', 2)

# Entra√Æner un √©pisode
stats = trainer.train_episode(player1, player2)
print(f"Gagnant: Joueur {stats['winner']}")
```

### Tournoi d'IA

```bash
cd magic_pong
python examples/ai_vs_ai.py --mode tournament
```

### Entra√Ænement Simple

```bash
cd magic_pong
python examples/ai_vs_ai.py --mode training
```

## Architecture

```
magic_pong/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/           # Moteur de jeu et physique
‚îÇ   ‚îú‚îÄ‚îÄ ai/             # Interface IA et exemples
‚îÇ   ‚îú‚îÄ‚îÄ graphics/       # Rendu graphique (√† venir)
‚îÇ   ‚îî‚îÄ‚îÄ utils/          # Configuration et utilitaires
‚îú‚îÄ‚îÄ examples/           # Exemples d'utilisation
‚îî‚îÄ‚îÄ docs/              # Documentation
```

### Composants Principaux

- **PhysicsEngine** : G√®re la physique du jeu, collisions, bonus
- **GameEngine** : Orchestre le jeu et g√®re les joueurs
- **TrainingManager** : Optimis√© pour l'entra√Ænement d'IA
- **AIPlayer** : Interface abstraite pour les IA
- **GameEnvironment** : Environnement compatible avec les frameworks RL

## Configuration

Le jeu est hautement configurable via [`src/utils/config.py`](src/utils/config.py):

```python
from src.utils.config import game_config, ai_config

# Configuration du jeu
game_config.FIELD_WIDTH = 800
game_config.FIELD_HEIGHT = 600
game_config.BALL_SPEED = 300.0

# Configuration IA
ai_config.HEADLESS_MODE = True
ai_config.FAST_MODE_MULTIPLIER = 10.0
```

## Cr√©er une IA Personnalis√©e

```python
from src.ai.interface import AIPlayer
from src.core.entities import Action

class MonIA(AIPlayer):
    def get_action(self, observation):
        # Votre logique ici
        ball_pos = observation['ball_pos']
        player_pos = observation['player_pos']

        # Calculer l'action
        move_x = ball_pos[0] - player_pos[0]
        move_y = ball_pos[1] - player_pos[1]

        return Action(move_x, move_y)

    def on_step(self, observation, action, reward, done, info):
        # Apprentissage ici
        self.current_episode_reward += reward
```

## Interface avec PyTorch

```python
import torch
import torch.nn as nn
from src.ai.interface import AIPlayer

class PyTorchAI(AIPlayer):
    def __init__(self, player_id, model):
        super().__init__(player_id)
        self.model = model

    def get_action(self, observation):
        # Convertir l'observation en tensor
        state = self._obs_to_tensor(observation)

        # Pr√©diction du mod√®le
        with torch.no_grad():
            action_probs = self.model(state)

        # Convertir en Action
        return Action(
            move_x=action_probs[0].item(),
            move_y=action_probs[1].item()
        )
```

## Observations pour l'IA

L'observation fournie √† chaque IA contient :

```python
observation = {
    'ball_pos': [x, y],                    # Position de la balle
    'ball_vel': [vx, vy],                  # V√©locit√© de la balle
    'player_pos': [x, y],                  # Position du joueur
    'opponent_pos': [x, y],                # Position de l'adversaire
    'player_paddle_size': float,           # Taille de la raquette
    'opponent_paddle_size': float,         # Taille raquette adverse
    'bonuses': [[x, y, type], ...],        # Bonus actifs
    'rotating_paddles': [[x, y, angle]], # Raquettes tournantes
    'score_diff': int,                     # Diff√©rence de score
    'time_elapsed': float                  # Temps √©coul√©
}
```

## Syst√®me de R√©compenses

- **+1.0** : Marquer un point
- **-1.0** : Encaisser un point
- **+0.1** : Collecter un bonus
- **+0.01** : Toucher la balle
- **+0.02** : Utiliser une raquette tournante

## Performance

En mode headless avec acc√©l√©ration :
- **Vitesse normale** : ~60 FPS
- **Mode rapide** : ~600-6000 FPS (10-100x plus rapide)
- **Entra√Ænement** : Milliers d'√©pisodes par minute

## Exemples de R√©sultats

Tournoi entre les IA incluses (20 parties chacune) :

```
Classement:
1. aggressive: 52 victoires
2. predictive: 48 victoires
3. defensive: 31 victoires
4. follow_ball: 28 victoires
5. random: 1 victoire
```

## D√©veloppement

### Structure du Code

- **S√©paration claire** entre logique m√©tier et affichage
- **Architecture modulaire** et extensible
- **Type hints** complets pour une meilleure maintenance
- **Tests unitaires** (√† venir)

### Ajouter de Nouveaux Bonus

```python
# Dans entities.py
class BonusType(Enum):
    MON_BONUS = "mon_bonus"

# Dans physics.py
def _apply_bonus_effect(self, bonus_type, player):
    if bonus_type == BonusType.MON_BONUS:
        # Votre effet ici
        pass
```

## Roadmap

- [ ] Interface graphique Pygame
- [ ] Mode multijoueur en r√©seau
- [ ] Int√©gration Gymnasium
- [ ] Sauvegarde/chargement de mod√®les
- [ ] M√©triques avanc√©es et visualisations
- [ ] Support GPU pour l'entra√Ænement

## Contribution

Les contributions sont les bienvenues ! Voir [`CONTRIBUTING.md`](CONTRIBUTING.md) pour les guidelines.

## Licence

MIT License - voir [`LICENSE`](LICENSE) pour les d√©tails.

## Auteur

Adrien Berchet - Projet Magic Pong pour l'entra√Ænement d'IA

---

**Magic Pong** - O√π l'IA apprend √† jouer ! üèìü§ñ
